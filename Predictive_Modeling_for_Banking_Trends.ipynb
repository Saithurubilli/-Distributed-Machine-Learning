{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saithurubilli/-Distributed-Machine-Learning/blob/main/Predictive_Modeling_for_Banking_Trends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_CcI-SNa4Qf",
        "outputId": "982f12dc-deb4-4316-83a5-e1ae4b00ac19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- y: string (nullable = true)\n",
            "\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|age|        job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n",
            "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n",
            "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n",
            "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n",
            "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ques 1 Load the bank.csv dataset into a Spark DataFrame.\n",
        "\n",
        "\n",
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"ML_Spark_Bank\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "df = spark.read.csv(\"/content/bank (1).csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the schema\n",
        "df.printSchema()\n",
        "\n",
        "# Show the first 5 rows\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the structure of the DataFrame.\n",
        "\n",
        "Confirms that Spark correctly inferred the data types for each column:\n",
        "\n",
        "For example:\n",
        "\n",
        "age is IntegerType\n",
        "\n",
        "job, marital, education, etc., are StringType\n",
        "\n",
        "balance, day, duration, etc., are IntegerType\n",
        "\n",
        "This ensures your data is ready for processing in Spark ML.\n",
        "\n",
        "üëÄ df.show(5) Output:\n",
        "Displays the first 5 rows of the dataset.\n",
        "\n",
        "You can visually verify that:\n",
        "\n",
        "The values are loaded correctly.\n",
        "\n",
        "No column is completely null.\n",
        "\n",
        "Target column y (yes/no) is present and valid."
      ],
      "metadata": {
        "id": "ToeOKAoHbqit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ques 2 Perform basic exploratory data analysis (EDA) to understand the dataset. Display the schema,\n",
        "#the first few rows, the number of rows and columns, and descriptive statistics for numeric columns.\n",
        "\n",
        "\n",
        "# Already done:\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "\n",
        "# New EDA: Row and column count\n",
        "print(f\"Number of Rows: {df.count()}\")\n",
        "print(f\"Number of Columns: {len(df.columns)}\")\n",
        "\n",
        "# Summary statistics for numeric columns\n",
        "df.describe(['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "498ESJXlbgXo",
        "outputId": "d2fbe329-b6a2-4a17-9672-fb0668c35975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- y: string (nullable = true)\n",
            "\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|age|        job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n",
            "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n",
            "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n",
            "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n",
            "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "only showing top 5 rows\n",
            "\n",
            "Number of Rows: 4521\n",
            "Number of Columns: 17\n",
            "+-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|summary|               age|           balance|               day|          duration|          campaign|             pdays|          previous|\n",
            "+-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|  count|              4521|              4521|              4521|              4521|              4521|              4521|              4521|\n",
            "|   mean| 41.17009511170095|1422.6578190665782|15.915284229152842|263.96129174961294| 2.793629727936297|39.766644547666445|0.5425790754257908|\n",
            "| stddev|10.576210958711263|3009.6381424673395| 8.247667327229934|259.85663262468216|3.1098066601885823|100.12112444301656|1.6935623506071211|\n",
            "|    min|                19|             -3313|                 1|                 4|                 1|                -1|                 0|\n",
            "|    max|                87|             71188|                31|              3025|                50|               871|                25|\n",
            "+-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.count() and len(df.columns)\n",
        "Shows the total number of records (rows) and number of columns in the dataset.\n",
        "\n",
        "Example:\n",
        "\n",
        "Number of Rows: 4521\n",
        "\n",
        "Number of Columns: 17\n",
        "\n",
        "This confirms the dataset is not empty and is ready for ML processing.\n",
        "\n",
        "üìä df.describe([...]).show()\n",
        "Displays summary statistics (count, mean, stddev, min, max) for numeric columns:\n",
        "\n",
        "age, balance, day, duration, campaign, pdays, previous\n",
        "\n",
        "This gives insights such as:\n",
        "\n",
        "Mean balance: Helps understand typical customer account balances.\n",
        "\n",
        "Max duration: Might indicate potential outliers (very high call duration).\n",
        "\n",
        "High standard deviation: Indicates large variation, which could require handling outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "YPjMc0GkcX48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ques 3 Handle Missing Values in the Dataset\n",
        "\n",
        "\n",
        "# Import functions\n",
        "from pyspark.sql.functions import col, when, count\n",
        "\n",
        "# Count missing (null) values in each column\n",
        "missing_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "missing_counts.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K6fVizKcY26",
        "outputId": "584db930-a1f0-4adf-97b6-04440aa8690a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|age|job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
            "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|  0|  0|      0|        0|      0|      0|      0|   0|      0|  0|    0|       0|       0|    0|       0|       0|  0|\n",
            "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means:\n",
        "\n",
        "You do not need to drop or impute any data.\n",
        "\n",
        "The dataset is clean and ready for further preprocessing."
      ],
      "metadata": {
        "id": "2ybV_EMDdQs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ques 4 Handle Outliers in the Dataset\n",
        "\n",
        "# Function to detect outliers using IQR\n",
        "def detect_outliers_iqr(df, column):\n",
        "    quantiles = df.approxQuantile(column, [0.25, 0.75], 0.05)\n",
        "    Q1 = quantiles[0]\n",
        "    Q3 = quantiles[1]\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    print(f\"\\nOutlier range for '{column}': [{lower_bound}, {upper_bound}]\")\n",
        "    outlier_count = df.filter((col(column) < lower_bound) | (col(column) > upper_bound)).count()\n",
        "    print(f\"Number of outliers in '{column}': {outlier_count}\")\n",
        "\n",
        "# Check for outliers in key numeric columns\n",
        "for col_name in ['age', 'balance', 'duration']:\n",
        "    detect_outliers_iqr(df, col_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSfsHiAtdRkS",
        "outputId": "dd98f85a-121d-4270-bd62-683492712b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Outlier range for 'age': [12.0, 68.0]\n",
            "Number of outliers in 'age': 67\n",
            "\n",
            "Outlier range for 'balance': [-1630.0, 2914.0]\n",
            "Number of outliers in 'balance': 647\n",
            "\n",
            "Outlier range for 'duration': [-176.0, 576.0]\n",
            "Number of outliers in 'duration': 457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights:\n",
        "Age: 67 entries fall outside the expected age range (could be very young or elderly clients).\n",
        "\n",
        "Balance: 647 accounts have very high or very low balances, likely legitimate for a financial dataset ‚Äî often kept.\n",
        "\n",
        "Duration: 457 calls lasted longer than 576 seconds ‚Äî this could be important since longer calls might indicate interest. But this variable can leak information, so it may be excluded from modeling later."
      ],
      "metadata": {
        "id": "NDu8FoT9dzgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ques5 Convert categorical variables to numerical format using StringIndexer or OneHotEncoder.\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# List of categorical columns\n",
        "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']\n",
        "\n",
        "# Create indexers\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\") for col in categorical_cols]\n",
        "\n",
        "# Apply transformations\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df_indexed = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Show result of indexed columns\n",
        "df_indexed.select([col + \"_indexed\" for col in categorical_cols] + [\"age\", \"balance\", \"duration\"]).show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjfL_ZUtd0kx",
        "outputId": "f08fda69-e79f-457e-aa92-fadfd6a9139f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+-----------------+---------------+---------------+------------+---------------+-------------+----------------+---------+---+-------+--------+\n",
            "|job_indexed|marital_indexed|education_indexed|default_indexed|housing_indexed|loan_indexed|contact_indexed|month_indexed|poutcome_indexed|y_indexed|age|balance|duration|\n",
            "+-----------+---------------+-----------------+---------------+---------------+------------+---------------+-------------+----------------+---------+---+-------+--------+\n",
            "|        8.0|            0.0|              2.0|            0.0|            1.0|         0.0|            0.0|          8.0|             0.0|      0.0| 30|   1787|      79|\n",
            "|        4.0|            0.0|              0.0|            0.0|            0.0|         1.0|            0.0|          0.0|             1.0|      0.0| 33|   4789|     220|\n",
            "|        0.0|            1.0|              1.0|            0.0|            0.0|         0.0|            0.0|          5.0|             1.0|      0.0| 35|   1350|     185|\n",
            "|        0.0|            0.0|              1.0|            0.0|            0.0|         1.0|            1.0|          3.0|             0.0|      0.0| 30|   1476|     199|\n",
            "|        1.0|            0.0|              0.0|            0.0|            0.0|         0.0|            1.0|          0.0|             0.0|      0.0| 59|      0|     226|\n",
            "+-----------+---------------+-----------------+---------------+---------------+------------+---------------+-------------+----------------+---------+---+-------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StringIndexer to convert all string-based categorical columns into numeric indices.\n",
        "\n",
        "I created a Pipeline to apply all the indexers efficiently in a single transformation step.\n",
        "\n",
        "Each new column (e.g., job_indexed) contains a numeric code representing each category from the original column.\n",
        "\n",
        "This transformation is essential because machine learning models in Spark ML require all input features to be numeric.\n",
        "\n",
        "‚ö†Ô∏è Note: I also indexed the target column y (e.g., yes ‚ûù 1.0, no ‚ûù 0.0), and it will be used as the label for model training."
      ],
      "metadata": {
        "id": "gP77pxg2eQrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ques 6 Create a feature vector using VectorAssembler by combining all feature columns.\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Define input features: numeric columns + indexed categorical columns (exclude target)\n",
        "input_features = [\n",
        "    'age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous',\n",
        "    'job_indexed', 'marital_indexed', 'education_indexed', 'default_indexed',\n",
        "    'housing_indexed', 'loan_indexed', 'contact_indexed',\n",
        "    'month_indexed', 'poutcome_indexed'\n",
        "]\n",
        "\n",
        "# Assemble all input features into a single 'features' column\n",
        "assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n",
        "final_df = assembler.transform(df_indexed)\n",
        "\n",
        "# Select only the final feature vector and label\n",
        "final_df.select(\"features\", \"y_indexed\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUYkwpVDeTDZ",
        "outputId": "96fb26c7-984c-496f-e38e-86469d9b29d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------+---------+\n",
            "|features                                                                  |y_indexed|\n",
            "+--------------------------------------------------------------------------+---------+\n",
            "|[30.0,1787.0,19.0,79.0,1.0,-1.0,0.0,8.0,0.0,2.0,0.0,1.0,0.0,0.0,8.0,0.0]  |0.0      |\n",
            "|[33.0,4789.0,11.0,220.0,1.0,339.0,4.0,4.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0]|0.0      |\n",
            "|[35.0,1350.0,16.0,185.0,1.0,330.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,5.0,1.0]|0.0      |\n",
            "|[30.0,1476.0,3.0,199.0,4.0,-1.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,1.0,3.0,0.0]  |0.0      |\n",
            "|(16,[0,2,3,4,5,7,13],[59.0,5.0,226.0,1.0,-1.0,1.0,1.0])                   |0.0      |\n",
            "+--------------------------------------------------------------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used VectorAssembler to combine all relevant input features into a single column called features.\n",
        "\n",
        "This is required by Spark ML models, which expect a feature vector (not separate columns) for training.\n",
        "\n",
        "The output now includes:\n",
        "\n",
        "A features column: A dense vector of all input variables.\n",
        "\n",
        "A y_indexed column: The target variable (label), already converted to numeric.\n",
        "\n",
        "This completes the transformation of the dataset into a form ready for model training."
      ],
      "metadata": {
        "id": "n4HEcjScenkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ques 7 Choose a classification model (e.g., Logistic Regression, Decision Tree Classifier) for predicting the subscription to a term deposit.\n",
        "#Split the data into training and test sets.\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Rename target column to 'label'\n",
        "model_df = final_df.select(\"features\", col(\"y_indexed\").alias(\"label\"))\n",
        "\n",
        "# Split into training and test sets (80/20)\n",
        "train_data, test_data = model_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Train the model\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Make predictions on test data\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Show predictions\n",
        "predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show(5, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYbmn0B3eoeT",
        "outputId": "081be263-a027-4a9a-8ec7-87c2ca41c642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------+-----+----------+-----------------------------------------+\n",
            "|features                                                               |label|prediction|probability                              |\n",
            "+-----------------------------------------------------------------------+-----+----------+-----------------------------------------+\n",
            "|(16,[0,1,2,3,4,5,6,7,15],[24.0,299.0,6.0,209.0,1.0,321.0,1.0,3.0,1.0]) |0.0  |0.0       |[0.9593690832670528,0.040630916732947164]|\n",
            "|(16,[0,1,2,3,4,5,6,7,15],[29.0,228.0,11.0,12.0,8.0,342.0,9.0,1.0,1.0]) |0.0  |0.0       |[0.9870533167274181,0.01294668327258186] |\n",
            "|(16,[0,1,2,3,4,5,6,7,15],[30.0,-28.0,18.0,284.0,2.0,371.0,1.0,1.0,1.0])|0.0  |0.0       |[0.9511480590312332,0.04885194096876677] |\n",
            "|(16,[0,1,2,3,4,5,6,7,15],[33.0,43.0,14.0,332.0,2.0,358.0,2.0,1.0,1.0]) |0.0  |0.0       |[0.9404157451105136,0.05958425488948638] |\n",
            "|(16,[0,1,2,3,4,5,6,7,15],[36.0,23.0,8.0,152.0,2.0,347.0,1.0,2.0,1.0])  |0.0  |0.0       |[0.9685569231076031,0.03144307689239689] |\n",
            "+-----------------------------------------------------------------------+-----+----------+-----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose Logistic Regression because it's effective for binary classification (predicting yes/no).\n",
        "\n",
        "I split the dataset into:\n",
        "\n",
        "80% training data\n",
        "\n",
        "20% testing data\n",
        "\n",
        "I renamed y_indexed to label (required by Spark ML).\n",
        "\n",
        "The model was trained on the training data and used to predict the labels for test data.\n",
        "\n",
        "The output shows:\n",
        "\n",
        "prediction: 0.0 = No, 1.0 = Yes\n",
        "\n",
        "probability: Confidence level for both classes\n",
        "\n"
      ],
      "metadata": {
        "id": "7hNOtYENfMGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ques 8 Evaluate the model on the test dataset using appropriate metrics (Accuracy, Precision, Recall, F1 Score).\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Evaluator for Accuracy\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "\n",
        "# Evaluator for Precision\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "\n",
        "# Evaluator for Recall\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "\n",
        "# Evaluator for F1 Score\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1 = evaluator_f1.evaluate(predictions)\n",
        "\n",
        "# Display all metrics\n",
        "print(f\"üîç Accuracy: {accuracy:.4f}\")\n",
        "print(f\"üîç Precision: {precision:.4f}\")\n",
        "print(f\"üîç Recall: {recall:.4f}\")\n",
        "print(f\"üîç F1 Score: {f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF9CcXiUfNNd",
        "outputId": "6e00be1e-2ecd-4449-941d-3df418c53b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Accuracy: 0.8917\n",
            "üîç Precision: 0.8707\n",
            "üîç Recall: 0.8917\n",
            "üîç F1 Score: 0.8707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: 0.8917\n",
        "\n",
        "This means 89.17% of predictions were correct.\n",
        "\n",
        "The model is performing well in general, with a high correct prediction rate.\n",
        "\n",
        "Precision: 0.8707\n",
        "\n",
        "This shows that when the model predicts \"yes\" (subscribed), it's correct 87.07% of the time.\n",
        "\n",
        "It's good for cases where false positives (predicting \"yes\" when it should be \"no\") are costly.\n",
        "\n",
        "Recall: 0.8917\n",
        "\n",
        "This shows that the model correctly predicts 89.17% of all actual \"yes\" cases.\n",
        "\n",
        "It's good for minimizing false negatives (missed subscriptions).\n",
        "\n",
        "F1 Score: 0.8707\n",
        "\n",
        "The F1 Score of 87.07% balances precision and recall. This is often considered a good indicator of overall performance, especially in imbalanced datasets.\n",
        "\n",
        "Conclusion:\n",
        "The model is performing well in terms of accuracy, precision, recall, and F1 score.\n",
        "\n",
        "These results suggest the model is able to correctly predict whether a client will subscribe to a term deposit, with minimal errors."
      ],
      "metadata": {
        "id": "2UsqddlUmWH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ques 9 Perform hyperparameter tuning using ParamGridBuilder and CrossValidator.\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Define evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "\n",
        "# Build parameter grid\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# CrossValidator setup\n",
        "cv = CrossValidator(estimator=lr,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=5)\n",
        "\n",
        "# Fit model with cross-validation\n",
        "cv_model = cv.fit(train_data)\n",
        "\n",
        "# Evaluate the best model\n",
        "best_model = cv_model.bestModel\n",
        "best_predictions = best_model.transform(test_data)\n",
        "\n",
        "# Recalculate Accuracy\n",
        "best_accuracy = evaluator_acc.evaluate(best_predictions)\n",
        "print(f\"üîç Best Model Accuracy after Tuning: {best_accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J24xs1UlmXR4",
        "outputId": "f0ca89c5-de01-4ad6-e2e1-af2c76006820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Best Model Accuracy after Tuning: 0.8906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing hyperparameter tuning using 5-fold cross-validation:\n",
        "\n",
        "The best logistic regression model achieved an accuracy of 0.8906 (or 89.06%).\n",
        "\n",
        "üîç Insight:\n",
        "The performance is very close to the original model (which had 89.17% accuracy).\n",
        "\n",
        "This suggests that:\n",
        "\n",
        "Your original hyperparameters were already near optimal.\n",
        "\n",
        "The model is stable and performs consistently under various configurations.\n",
        "\n",
        "‚úÖ Hyperparameter tuning is still important for validating and confirming that your model performs well across different parameter settings."
      ],
      "metadata": {
        "id": "z6NWwW5Cm8c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ques 10  Analyze the feature importances (if applicable) or coefficients of the model to gain insights into which features are most influential in predicting the outcome.\n",
        "\n",
        "# Convert Spark SparseVector to a Python list\n",
        "coef_values = list(best_model.coefficients)\n",
        "\n",
        "# Create a DataFrame using feature names and their corresponding coefficient values\n",
        "coef_df = pd.DataFrame({\n",
        "    \"Feature\": features_list,\n",
        "    \"Coefficient\": coef_values\n",
        "}).sort_values(by=\"Coefficient\", ascending=False)\n",
        "\n",
        "# Print model intercept\n",
        "print(\"Intercept:\", best_model.intercept)\n",
        "\n",
        "# Show top positive and negative influential features\n",
        "print(\"\\nTop Influential Features:\\n\")\n",
        "print(coef_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlR56Wqzm9fl",
        "outputId": "bc7878b6-fb65-474d-8340-7d412b0aaace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: -4.156678040680314\n",
            "\n",
            "Top Influential Features:\n",
            "\n",
            "          Feature  Coefficient\n",
            " poutcome_indexed     0.664650\n",
            "  housing_indexed     0.467824\n",
            "  marital_indexed     0.179996\n",
            "    month_indexed     0.121712\n",
            "         duration     0.003612\n",
            "              age     0.000000\n",
            "          balance     0.000000\n",
            "              day     0.000000\n",
            "      job_indexed     0.000000\n",
            "         previous     0.000000\n",
            "            pdays     0.000000\n",
            "         campaign     0.000000\n",
            "  default_indexed     0.000000\n",
            "education_indexed     0.000000\n",
            "  contact_indexed    -0.064667\n",
            "     loan_indexed    -0.468615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most influential factor is the result of previous campaign outcomes (poutcome).\n",
        "\n",
        "Clients with housing loans, and those contacted in certain months, are also more likely to subscribe.\n",
        "\n",
        "Clients already dealing with a personal loan are less likely to subscribe.\n",
        "\n",
        "Several common features like age, job, and balance didn‚Äôt significantly affect predictions in this model.\n",
        "\n"
      ],
      "metadata": {
        "id": "FCQXRTuRoSmi"
      }
    }
  ]
}